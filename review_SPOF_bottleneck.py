# -----------------------------
# graph_review_spof_bottleneck_v3.py
# ëª©ì :
# 1) DBì—ì„œ ê°€ì¥ ìµœê·¼ submissionì˜ mermaid_textë¥¼ ê°€ì ¸ì˜´
# 2) Mermaid í…ìŠ¤íŠ¸ë¥¼ "ë…¸ë“œ/ì—£ì§€(ê·¸ë˜í”„)" êµ¬ì¡°ë¡œ íŒŒì‹±
# 3) SPOF í›„ë³´(ë‹¨ì ˆì ) / ë³‘ëª© í›„ë³´(ì¤‘ì•™ì„±+fan-in) ê³„ì‚°
# 4) ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ì œì‹œ
# 5) ë™ì  Follow-up ì§ˆë¬¸ ìƒì„±
# 6) system_results.score_breakdown_jsonì— graph_analysis ì¶”ê°€
# 7) SPOF/ë³‘ëª©ì— ë”°ë¼ score_total ê°ì  ë°˜ì˜ + risk_flags ì¶”ê°€
# ì „ì œ:
# - 03_demo_submission_result.sqlë¡œ system_results rowê°€ ì´ë¯¸ ìƒì„±ë¼ ìˆì–´ì•¼ í•¨
# ê°œì„ ì‚¬í•­:
# - ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ìë™ ì œì‹œ
# - ë™ì  ì§ˆë¬¸ ìƒì„± (SPOF/ë³‘ëª©/Tradeoffë³„)
# - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
# - í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ DB ì—°ê²°
# - Mermaid íŒŒì‹± ì •í™•ë„ í–¥ìƒ
# -----------------------------

import re
import json
import os
import sys
from typing import Dict, List, Tuple, Set, Optional
from dotenv import load_dotenv

import mysql.connector
import networkx as nx


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# [MODIFIED] ê°ì  ì •ì±…
SPOF_PENALTY_PER = 12
SPOF_PENALTY_CAP = 36
BOTTLENECK_PENALTY_PER = 6
BOTTLENECK_PENALTY_CAP = 18


# Mermaid íŒŒì‹±ìš© ì •ê·œì‹
LABEL_DEF_RE = re.compile(r"([A-Za-z0-9_]+)\s*(\[[^\]]*\]|\([^\)]*\)|\{[^\}]*\})")

EDGE_LINE_RE = re.compile(
    r"^\s*([A-Za-z0-9_]+)(?:\[[^\]]*\]|\([^\)]*\)|\{[^\}]*\})?\s*[-.]*>\s*(?:\|[^|]*\|\s*)?([A-Za-z0-9_]+)"
)

NODE_ID_RE = re.compile(r"\b([A-Za-z0-9_]+)\b")

COMMENT_LINE_RE = re.compile(r"^\s*%%")


# ========== [NEW] ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ì œì‹œ ë¡œì§ ==========
def generate_alternative_architecture(
    spofs: List[str],
    bottlenecks: List[Dict],
    labels: Dict[str, str],
    G: nx.DiGraph
) -> str:
    """
    SPOF/ë³‘ëª©ì„ í•´ê²°í•˜ëŠ” ëŒ€ì•ˆ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì œì‹œ.
    
    ì „ëµ:
    1) SPOFê°€ ìˆìœ¼ë©´ â†’ ì´ì¤‘í™”/ë¡œë“œë°¸ëŸ°ì‹± ì œì•ˆ
    2) ë³‘ëª©ì´ ìˆìœ¼ë©´ â†’ íŒŒí‹°ì…”ë‹/ìƒ¤ë”©/ìºì‹± ì œì•ˆ
    3) ë‹¨ìˆœ êµ¬ì¡°ë©´ â†’ ê´€ì¸¡ì„±/FT ê°•í™” ì œì•ˆ
    """
    suggestions = []
    
    # SPOF í•´ê²° ë°©ì•ˆ
    if spofs:
        for spof_node in spofs:
            label = labels.get(spof_node, spof_node)
            
            # ë…¸ë“œ íƒ€ì… ì¶”ì •
            if any(k in label.lower() for k in ['gateway', 'lb', 'ingress']):
                suggestions.append(
                    f"âœ“ [{spof_node} ì´ì¤‘í™”] {label} ì•ì— ë¡œë“œë°¸ëŸ°ì„œ 2ëŒ€ ì´ìƒ ë°°ì¹˜ "
                    f"(Active-Active ë˜ëŠ” Active-Standby), í—¬ìŠ¤ì²´í¬ ê¸°ë°˜ í˜ì¼ì˜¤ë²„"
                )
            elif any(k in label.lower() for k in ['db', 'database', 'store']):
                suggestions.append(
                    f"âœ“ [{spof_node} ë ˆí”Œë¦¬ì¹´] {label} ë§ˆìŠ¤í„°-ìŠ¬ë ˆì´ë¸Œ êµ¬ì„± ë˜ëŠ” í´ëŸ¬ìŠ¤í„°(ìƒ¤ë”©), "
                    f"ì½ê¸°/ì“°ê¸° ë¶„ë¦¬ë¡œ ë¶€í•˜ ë¶„ì‚°"
                )
            elif any(k in label.lower() for k in ['broker', 'queue', 'kafka']):
                suggestions.append(
                    f"âœ“ [{spof_node} í´ëŸ¬ìŠ¤í„°] {label}ë¥¼ 3ê°œ ì´ìƒ ë…¸ë“œë¡œ êµ¬ì„±, "
                    f"íŒŒí‹°ì…˜ ìë™ ë¦¬ë°¸ëŸ°ì‹±"
                )
            elif any(k in label.lower() for k in ['worker', 'processor', 'consumer']):
                suggestions.append(
                    f"âœ“ [{spof_node} ìˆ˜í‰í™•ì¥] {label} ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰, "
                    f"ë¡œë“œë°¸ëŸ°ì„œ/ë©”ì‹œì§€ íë¡œ ë¶€í•˜ ë¶„ì‚°"
                )
            else:
                suggestions.append(
                    f"âœ“ [{spof_node} ì´ì¤‘í™”] {label}ë¥¼ ìµœì†Œ 2ê°œ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµ¬ì„±, "
                    f"ìë™ í˜ì¼ì˜¤ë²„ ë° í—¬ìŠ¤ì²´í¬ ì„¤ì •"
                )
    
    # ë³‘ëª© í•´ê²° ë°©ì•ˆ
    if bottlenecks:
        for bn in bottlenecks[:2]:  # ìƒìœ„ 2ê°œë§Œ
            node = bn["node"]
            label = bn.get("label", node)
            fanin = bn.get("fanin", 0)
            
            if fanin >= 3:
                suggestions.append(
                    f"âœ“ [{node} ìºì‹±] {label}ì˜ ì‘ë‹µì„ Redis/Memcachedë¡œ ìºì‹±, "
                    f"TTL ì •ì±…ìœ¼ë¡œ ì‹ ì„ ë„ ê´€ë¦¬"
                )
            
            if any(k in label.lower() for k in ['db', 'database']):
                suggestions.append(
                    f"âœ“ [{node} ìƒ¤ë”©] {label}ë¥¼ í•« ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìƒ¤ë”©, "
                    f"ë²”ìœ„/í•´ì‹œ ê¸°ë°˜ íŒŒí‹°ì…”ë‹"
                )
            elif any(k in label.lower() for k in ['service', 'api', 'handler']):
                suggestions.append(
                    f"âœ“ [{node} ë¹„ë™ê¸°í™”] {label}ì˜ ë¬´ê±°ìš´ ì‘ì—…ì„ íì— ì˜¤í”„ë¡œë“œ, "
                    f"ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ë¡œ ì²˜ë¦¬"
                )
    
    # ê¸°ë³¸ ì œì•ˆ
    if not suggestions:
        suggestions = [
            "âœ“ [ê´€ì¸¡ì„± ê°•í™”] Trace/Metric/Log í†µí•© ìˆ˜ì§‘ìœ¼ë¡œ ì¥ì•  ê·¼ì¸ ì¶”ì  ì†ë„ â†‘",
            "âœ“ [Circuit Breaker] ì¢…ì† ì„œë¹„ìŠ¤ ì¥ì•  ì‹œ ë¹ ë¥¸ í˜ì¼ì˜¤ë²„",
            "âœ“ [ì¬ì‹œë„ ì •ì±…] ì¼ì‹œì  ì˜¤ë¥˜ëŠ” ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„"
        ]
    
    return "\n".join(suggestions)


# ========== [NEW] ë™ì  Follow-up ì§ˆë¬¸ ìƒì„± ==========
def generate_followup_questions(
    submission: Dict,
    graph_analysis: Dict,
    penalty_info: Dict
) -> List[str]:
    """
    SPOF/ë³‘ëª©/Tradeoffì— ë”°ë¼ ì‹œë‹ˆì–´ ë©´ì ‘ê´€ ì§ˆë¬¸ ìë™ ìƒì„±.
    """
    questions = []
    
    # SPOF ì§ˆë¬¸
    if graph_analysis.get("spof_candidates"):
        spofs = graph_analysis["spof_candidates"]
        if len(spofs) == 1:
            q = f"'{spofs[0]}' ì»´í¬ë„ŒíŠ¸ê°€ ì¥ì•  ì‹œ, ì–´ë–»ê²Œ ì „ì²´ ì„œë¹„ìŠ¤ë¥¼ ë³´í˜¸í•  ê±´ê°€ìš”?"
        else:
            q = f"ì´ ì•„í‚¤í…ì²˜ì— {len(spofs)}ê°œ SPOFê°€ ìˆëŠ”ë°, ìµœìš°ì„  ì´ì¤‘í™” ëŒ€ìƒì€?"
        questions.append(q)
    
    # ë³‘ëª© ì§ˆë¬¸
    if graph_analysis.get("bottleneck_candidates"):
        bottlenecks = graph_analysis["bottleneck_candidates"]
        bn_top = bottlenecks[0]["node"] if bottlenecks else "ë³‘ëª©"
        questions.append(
            f"'{bn_top}' ë…¸ë“œì˜ ì²˜ë¦¬ëŸ‰(throughput)ì´ P99ì—ì„œ í­ì¦í•˜ë©´?"
        )
    
    # Tradeoff ì§ˆë¬¸
    tradeoffs_json = submission.get("tradeoffs_json")
    if isinstance(tradeoffs_json, str):
        tradeoffs_json = json.loads(tradeoffs_json)
    
    if tradeoffs_json:
        for i, td in enumerate(list(tradeoffs_json)[:2]):
            topic = td.get("topic", "ì„ íƒ")
            cons = td.get("cons", "ë‹¨ì ")
            questions.append(
                f"'{topic}' íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ '{cons}'ë¥¼ ì–´ë–»ê²Œ ì™„í™”í•  ê±´ê°€ìš”?"
            )
    
    # ì¼ë°˜ ì•„í‚¤í…ì²˜ ì§ˆë¬¸
    if len(questions) < 5:
        general_qs = [
            "ì´ ì„¤ê³„ì—ì„œ ê°€ì¥ ì·¨ì•½í•œ ë¶€ë¶„(Single Point of Concern)ì€ ì–´ë””ì¸ê°€ìš”?",
            "íŠ¸ë˜í”½ì´ 10ë°° ì¦ê°€í•˜ë©´, ì–´ë–¤ ì»´í¬ë„ŒíŠ¸ë¶€í„° ìŠ¤ì¼€ì¼ë§í•  ê±´ê°€ìš”?",
            "ì¥ì•  ë°œìƒ ì‹œ ë³µêµ¬ ìˆœì„œ(Recovery Order)ë¥¼ ì–´ë–»ê²Œ ì •í•  ê±´ê°€ìš”?",
            "ì´ ì•„í‚¤í…ì²˜ì˜ ë¹„ìš©ì€ ì–´ë–»ê²Œ ìµœì í™”í•  ìˆ˜ ìˆë‚˜ìš”?",
            "íŒ€ ê·œëª¨(SRE ëª‡ ëª…)ê°€ ìš´ì˜ ê°€ëŠ¥í•  ê²ƒ ê°™ë‚˜ìš”?"
        ]
        
        for gq in general_qs:
            if len(questions) < 5:
                questions.append(gq)
    
    return questions[:5]  # ìƒìœ„ 5ê°œë§Œ


# ========== ê¸°ì¡´ ì½”ë“œ (parse_annotations ~ compute_bottlenecks) ==========
def parse_annotations(mermaid_text: str) -> Tuple[Set[str], Optional[str], Optional[str]]:
    """Mermaid ì£¼ì„ íŒíŠ¸ íŒŒì‹±."""
    redundant: Set[str] = set()
    entry = None
    exit_ = None

    for raw in mermaid_text.splitlines():
        line = raw.strip()
        if not line.startswith("%%"):
            continue

        low = line.lower()

        if "redundant:" in low:
            part = line.split("redundant:", 1)[1]
            redundant |= {x.strip() for x in part.split(",") if x.strip()}

        if "entry:" in low:
            entry = line.split("entry:", 1)[1].strip()

        if "exit:" in low:
            exit_ = line.split("exit:", 1)[1].strip()

    return redundant, entry, exit_


def normalize_line(line: str) -> str:
    """ì—£ì§€ ë¼ë²¨ ì œê±° ë° ê³µë°± ì •ë¦¬."""
    line = re.sub(r"\|[^|]*\|", "", line)
    return line.strip()


def parse_mermaid_edges_and_labels(mermaid_text: str) -> Tuple[List[Tuple[str, str]], Dict[str, str]]:
    """Mermaid -> (edges, labels) íŒŒì‹±."""
    labels: Dict[str, str] = {}
    edges: List[Tuple[str, str]] = []

    for m in LABEL_DEF_RE.finditer(mermaid_text):
        node_id = m.group(1)
        raw_label = m.group(2).strip("[](){}")
        labels[node_id] = raw_label

    for raw in mermaid_text.splitlines():
        if COMMENT_LINE_RE.match(raw):
            continue

        line = raw.strip()
        if not line:
            continue

        low = line.lower()

        if low.startswith("graph ") or low.startswith("flowchart "):
            continue
        if low.startswith("subgraph") or low == "end":
            continue

        line = normalize_line(line)
        if not line:
            continue

        parts = [p.strip() for p in line.split(";") if p.strip()]

        for p in parts:
            m = EDGE_LINE_RE.match(p)
            if m:
                a, b = m.group(1), m.group(2)
                edges.append((a, b))
                continue

            if "-->" in p or "->" in p:
                tmp = re.sub(r"-{1,3}\.?->", " -> ", p)
                tokens = [t.strip() for t in tmp.split("->") if t.strip()]

                chain_nodes: List[str] = []
                for t in tokens:
                    found = NODE_ID_RE.findall(t)
                    if found:
                        chain_nodes.append(found[0])

                for i in range(len(chain_nodes) - 1):
                    edges.append((chain_nodes[i], chain_nodes[i + 1]))

    seen = set()
    uniq_edges: List[Tuple[str, str]] = []
    for e in edges:
        if e not in seen:
            uniq_edges.append(e)
            seen.add(e)

    return uniq_edges, labels


def choose_entry_exit(
    G: nx.DiGraph,
    labels: Dict[str, str],
    entry_hint: Optional[str],
    exit_hint: Optional[str]
) -> Tuple[Optional[str], List[str]]:
    """Entry/Exit ë…¸ë“œ ê²°ì •."""
    entry: Optional[str] = None
    exits: List[str] = []

    if entry_hint and entry_hint in G:
        entry = entry_hint
    else:
        candidates = [n for n in G.nodes if G.in_degree(n) == 0]
        for n in candidates:
            lab = (labels.get(n, "") or "").lower()
            if n.lower() in ("u", "user") or "user" in lab:
                entry = n
                break
        if not entry and candidates:
            entry = candidates[0]

    if exit_hint and exit_hint in G:
        exits = [exit_hint]
    else:
        exits = [n for n in G.nodes if G.out_degree(n) == 0]

        if not exits:
            key = ["db", "database", "vector", "llm", "model"]
            for n in G.nodes:
                lab = (labels.get(n, "") or "").lower()
                if any(k in lab for k in key):
                    exits.append(n)

    return entry, exits


def core_subgraph_nodes(G: nx.DiGraph, entry: Optional[str], exits: List[str]) -> Set[str]:
    """Entry -> Exit ê²½ë¡œ ìƒ ë…¸ë“œë“¤ë§Œ ì¶”ì¶œ."""
    if not entry or entry not in G or not exits:
        return set(G.nodes)

    reachable_from_entry = nx.descendants(G, entry) | {entry}
    RG = G.reverse(copy=False)

    core: Set[str] = set()
    for ex in exits:
        if ex not in G:
            continue
        can_reach_ex = nx.descendants(RG, ex) | {ex}
        core |= (reachable_from_entry & can_reach_ex)

    return core if core else set(G.nodes)


def compute_spof(
    G: nx.DiGraph,
    entry: Optional[str],
    exits: List[str],
    core_nodes: Set[str],
    redundant: Set[str]
) -> List[str]:
    """SPOF í›„ë³´ ê³„ì‚° (ë‹¨ì ˆì  ê¸°ë°˜)."""
    if not entry or entry not in G or not exits:
        return []

    H = G.subgraph(core_nodes).copy()
    UG = H.to_undirected()

    candidates = set(nx.articulation_points(UG))
    candidates -= {entry}
    candidates -= set(exits)
    candidates -= set(redundant)

    spofs: List[str] = []
    for node in candidates:
        H2 = H.copy()
        if node not in H2:
            continue
        H2.remove_node(node)

        cut = False
        for ex in exits:
            if ex not in H2:
                cut = True
                break
            if not nx.has_path(H2, entry, ex):
                cut = True
                break

        if cut:
            spofs.append(node)

    return spofs


def compute_bottlenecks(
    G: nx.DiGraph,
    core_nodes: Set[str],
    labels: Dict[str, str],
    topk: int = 3
) -> List[Dict]:
    """ë³‘ëª© í›„ë³´ ê³„ì‚° (ì¤‘ì•™ì„± + fan-in/out)."""
    H = G.subgraph(core_nodes).copy()
    if H.number_of_nodes() == 0:
        return []

    bc = nx.betweenness_centrality(H.to_undirected(), normalized=True)

    stateful_keys = ["db", "database", "vector", "redis", "queue", "kafka", "mq", "cache"]
    scored = []

    for n in H.nodes:
        lab = (labels.get(n, "") or "").lower()
        fanin = H.in_degree(n)
        fanout = H.out_degree(n)

        bonus = 0.0
        if any(k in lab for k in stateful_keys):
            bonus += 0.20

        score = bc.get(n, 0.0) + 0.06 * fanin + 0.02 * fanout + bonus
        scored.append((n, score, fanin, fanout, bc.get(n, 0.0)))

    scored.sort(key=lambda x: x[1], reverse=True)

    results: List[Dict] = []
    for n, score, fanin, fanout, bcv in scored[:topk]:
        results.append({
            "node": n,
            "label": labels.get(n),
            "score": round(score, 4),
            "fanin": fanin,
            "fanout": fanout,
            "betweenness": round(bcv, 4),
        })
    return results


def calc_penalties(spofs: List[str], bottlenecks: List[Dict]) -> Dict:
    """ê°ì  ê³„ì‚°."""
    spof_count = len(spofs)
    bottleneck_count = len(bottlenecks)

    spof_penalty = min(SPOF_PENALTY_CAP, spof_count * SPOF_PENALTY_PER)
    bottleneck_penalty = min(BOTTLENECK_PENALTY_CAP, bottleneck_count * BOTTLENECK_PENALTY_PER)

    total_penalty = spof_penalty + bottleneck_penalty

    return {
        "spof_count": spof_count,
        "bottleneck_count": bottleneck_count,
        "spof_penalty": spof_penalty,
        "bottleneck_penalty": bottleneck_penalty,
        "total_penalty": total_penalty,
        "policy": {
            "spof_per": SPOF_PENALTY_PER,
            "spof_cap": SPOF_PENALTY_CAP,
            "bottleneck_per": BOTTLENECK_PENALTY_PER,
            "bottleneck_cap": BOTTLENECK_PENALTY_CAP,
        }
    }


def update_system_results(
    conn,
    submission_id: int,
    graph_analysis: dict,
    penalty_info: dict,
    alternative_arch: str,
    questions: List[str]
):
    """
    [MODIFIED] system_results ì—…ë°ì´íŠ¸.
    - graph_analysis + penalty + ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ + ì§ˆë¬¸ ì €ì¥
    """
    try:
        cur = conn.cursor(dictionary=True)

        cur.execute(
            "SELECT score_total, score_breakdown_json, risk_flags_json, questions_json FROM system_results WHERE submission_id=%s",
            (submission_id,)
        )
        row = cur.fetchone()
        if not row:
            raise RuntimeError(
                "system_resultsê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 03_demo_submission_result.sqlì„ ì‹¤í–‰í•´ ê²°ê³¼ rowë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
            )

        old_score_total = row["score_total"]

        breakdown = row["score_breakdown_json"]
        flags = row["risk_flags_json"]

        if isinstance(breakdown, str):
            breakdown = json.loads(breakdown)
        if isinstance(flags, str):
            flags = json.loads(flags)

        total_penalty = penalty_info["total_penalty"]
        new_score_total = max(0, int(old_score_total) - int(total_penalty))

        breakdown.setdefault("items", {})
        breakdown.setdefault("meta", {})

        breakdown["items"]["graph_analysis"] = graph_analysis

        breakdown["meta"]["graph_penalty"] = {
            "old_score_total": old_score_total,
            "new_score_total": new_score_total,
            **penalty_info
        }

        if not isinstance(flags, list):
            flags = []

        flag_set = set(flags)

        if penalty_info["spof_count"] > 0:
            flag_set.add("SPOF_DETECTED")
            flag_set.add("SCORE_DEDUCTED_FOR_SPOF")

        if penalty_info["bottleneck_count"] > 0:
            flag_set.add("BOTTLENECK_CANDIDATES")
            flag_set.add("SCORE_DEDUCTED_FOR_BOTTLENECKS")

        if total_penalty > 0:
            flag_set.add("GRAPH_PENALTY_APPLIED")

        new_flags = list(flag_set)

        # [NEW] ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ + ì§ˆë¬¸ë„ ì €ì¥
        coach_summary = (
            f"ëŒ€ì•ˆ ì•„í‚¤í…ì²˜:\n{alternative_arch}\n\n"
            f"ì½”ì¹˜ ì§ˆë¬¸ (ë‹µë³€ í›„ ì¬ê²€í†  ê°€ëŠ¥):\n" +
            "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
        )

        cur.execute(
            "UPDATE system_results "
            "SET score_total=%s, score_breakdown_json=%s, risk_flags_json=%s, "
            "alternative_mermaid_text=%s, questions_json=%s, coach_summary=%s "
            "WHERE submission_id=%s",
            (
                new_score_total,
                json.dumps(breakdown, ensure_ascii=False),
                json.dumps(new_flags, ensure_ascii=False),
                alternative_arch,
                json.dumps(questions, ensure_ascii=False),
                coach_summary,
                submission_id
            )
        )
        conn.commit()
        
    except mysql.connector.Error as db_err:
        print(f"âŒ DB ì˜¤ë¥˜: {db_err}")
        raise
    finally:
        if cur:
            cur.close()


def get_db_connection():
    """í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ DB ì—°ê²° (ë³´ì•ˆ ê°•í™”)."""
    try:
        conn = mysql.connector.connect(
            host=os.getenv("DB_HOST", "localhost"),
            user=os.getenv("DB_USER", "root"),
            password=os.getenv("DB_PASSWORD", ""),
            database="Engineer_GYM",
            autocommit=False
        )
        return conn
    except mysql.connector.Error as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ .env íŒŒì¼ì— ë‹¤ìŒì„ ì„¤ì •í•˜ì„¸ìš”:")
        print("DB_HOST=localhost")
        print("DB_USER=root")
        print("DB_PASSWORD=your_password")
        sys.exit(1)


def main():
    """[MODIFIED] ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° ëŒ€ì•ˆ ìƒì„±."""
    try:
        conn = get_db_connection()
        cur = conn.cursor(dictionary=True)

        # (1) ê°€ì¥ ìµœê·¼ ì œì¶œ 1ê±´
        cur.execute("SELECT * FROM system_submissions ORDER BY id DESC LIMIT 1")
        sub = cur.fetchone()
        if not sub:
            print("âŒ ë¶„ì„í•  submissionì´ ì—†ìŠµë‹ˆë‹¤. 03_demo_submission_result.sqlì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return 1

        submission_id = sub["id"]
        mermaid_text = sub["mermaid_text"]

        print(f"ğŸ“Š ë¶„ì„ ì‹œì‘: submission_id={submission_id}")

        # (2) Mermaid íŒŒì‹±
        redundant, entry_hint, exit_hint = parse_annotations(mermaid_text)
        edges, labels = parse_mermaid_edges_and_labels(mermaid_text)

        if not edges:
            print("âš ï¸  ê²½ê³ : íŒŒì‹±ëœ ì—£ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. Mermaid í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")

        # (3) ê·¸ë˜í”„ ìƒì„±
        G = nx.DiGraph()
        G.add_edges_from(edges)

        # (4) Entry/Exit, Core ì¶”ì¶œ
        entry, exits = choose_entry_exit(G, labels, entry_hint, exit_hint)
        core = core_subgraph_nodes(G, entry, exits)

        # (5) SPOF / ë³‘ëª© ê³„ì‚°
        spofs = compute_spof(G, entry, exits, core, redundant)
        bottlenecks = compute_bottlenecks(G, core, labels, topk=3)

        # (6) ê°ì  ê³„ì‚°
        penalty_info = calc_penalties(spofs, bottlenecks)

        # [NEW] (7) ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ ìƒì„±
        alternative_arch = generate_alternative_architecture(spofs, bottlenecks, labels, G)

        # [NEW] (8) ë™ì  ì§ˆë¬¸ ìƒì„±
        questions = generate_followup_questions(sub, graph_analysis={
            "spof_candidates": spofs,
            "bottleneck_candidates": bottlenecks,
            "nodes_cnt": len(G.nodes),
            "edges_cnt": len(G.edges),
        }, penalty_info=penalty_info)

        # (9) graph_analysis JSON
        graph_analysis = {
            "entry": entry,
            "exits": exits,
            "nodes_cnt": len(G.nodes),
            "edges_cnt": len(G.edges),
            "core_nodes_cnt": len(core),
            "redundant_marked": sorted(list(redundant)),
            "spof_candidates": spofs,
            "bottleneck_candidates": bottlenecks,
            "notes": "v3: SPOF/ë³‘ëª© íƒì§€ + ëŒ€ì•ˆ ì•„í‚¤í…ì²˜ + ë™ì  ì§ˆë¬¸ ìƒì„±"
        }

        # (10) DB ì—…ë°ì´íŠ¸ (ëŒ€ì•ˆ + ì§ˆë¬¸ í¬í•¨)
        update_system_results(
            conn,
            submission_id,
            graph_analysis,
            penalty_info,
            alternative_arch,
            questions
        )

        # (11) ê²°ê³¼ ì¶œë ¥
        print("\nâœ… system_results ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        print(f"ğŸ“Œ submission_id: {submission_id}")
        print(f"ğŸ”´ SPOF í›„ë³´: {len(spofs)}ê°œ â†’ ê°ì  {penalty_info['spof_penalty']}")
        print(f"ğŸŸ  ë³‘ëª© í›„ë³´: {len(bottlenecks)}ê°œ â†’ ê°ì  {penalty_info['bottleneck_penalty']}")
        print(f"ğŸ“Š ì´ ê°ì : {penalty_info['total_penalty']}")
        print("\nğŸ’¡ ëŒ€ì•ˆ ì•„í‚¤í…ì²˜:")
        print(alternative_arch)
        print("\nâ“ Follow-up ì§ˆë¬¸:")
        for i, q in enumerate(questions, 1):
            print(f"{i}. {q}")
        
        return 0

    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        if conn:
            conn.close()


if __name__ == "__main__":
    sys.exit(main())


# -----------------------------
# ========== ê¸°ì¡´ ì½”ë“œ (parse_annotations ~ compute_bottlenecks) ==========
# [A] ê°ì  ì •ì±…(ì›í•˜ì‹œë©´ ì—¬ê¸°ë§Œ ì¡°ì •)
# -----------------------------
SPOF_PENALTY_PER = 12          # SPOF í›„ë³´ 1ê°œë‹¹ ê°ì 
SPOF_PENALTY_CAP = 36          # SPOF ê°ì  ìµœëŒ€ì¹˜
BOTTLENECK_PENALTY_PER = 6     # ë³‘ëª© í›„ë³´ 1ê°œë‹¹ ê°ì 
BOTTLENECK_PENALTY_CAP = 18    # ë³‘ëª© ê°ì  ìµœëŒ€ì¹˜


# -----------------------------
# [B] Mermaid íŒŒì‹±ìš© ì •ê·œì‹(Flowchart MVP ë²„ì „)
# -----------------------------
# ë…¸ë“œ ë¼ë²¨ ì •ì˜: A[Label], A(Label), A{Label}
LABEL_DEF_RE = re.compile(r"([A-Za-z0-9_]+)\s*(\[[^\]]*\]|\([^\)]*\)|\{[^\}]*\})")

# ë¼ì¸ ë‹¨ìœ„ ì—£ì§€: A --> B, A -->|text| B í˜•íƒœë¥¼ ìš°ì„  ì§€ì›
EDGE_LINE_RE = re.compile(
    r"^\s*([A-Za-z0-9_]+)(?:\[[^\]]*\]|\([^\)]*\)|\{[^\}]*\})?\s*[-.]*>\s*(?:\|[^|]*\|\s*)?([A-Za-z0-9_]+)"
)

# ì²´ì¸ íŒŒì‹± ì‹œ ë…¸ë“œ IDë¥¼ ë½‘ê¸° ìœ„í•œ ë§¤ìš° ë‹¨ìˆœí•œ íŒ¨í„´
NODE_ID_RE = re.compile(r"\b([A-Za-z0-9_]+)\b")

# Mermaid ì£¼ì„ ë¼ì¸(%%ë¡œ ì‹œì‘)
COMMENT_LINE_RE = re.compile(r"^\s*%%")


# -----------------------------
# [C] Mermaid ì£¼ì„ íŒíŠ¸ íŒŒì‹±
# -----------------------------
def parse_annotations(mermaid_text: str) -> Tuple[Set[str], Optional[str], Optional[str]]:
    """
    Mermaid ì£¼ì„(%%)ì— ì•„ë˜ íŒíŠ¸ë¥¼ ì ì–´ë‘ë©´ ì •í™•ë„ê°€ ì¢‹ì•„ì§‘ë‹ˆë‹¤:
    - %% redundant: G,R  => G, Rì€ ì´ì¤‘í™”ë¡œ ê°„ì£¼í•˜ì—¬ SPOF í›„ë³´ì—ì„œ ì œì™¸
    - %% entry: U        => entry ë…¸ë“œ ì§€ì •(ì‹œì‘ì )
    - %% exit: V         => exit ë…¸ë“œ ì§€ì •(ì¢…ì°©ì )
    
    ì˜ˆì‹œ:
      %% entry: Client
      %% redundant: LB1,LB2,Cache
      %% exit: DB
      graph TD
        Client[Client] --> LB1[Load Balancer 1]
        ...
    """
    redundant: Set[str] = set()
    entry = None
    exit_ = None

    for raw in mermaid_text.splitlines():
        line = raw.strip()
        if not line.startswith("%%"):
            continue

        low = line.lower()

        if "redundant:" in low:
            part = line.split("redundant:", 1)[1]
            redundant |= {x.strip() for x in part.split(",") if x.strip()}

        if "entry:" in low:
            entry = line.split("entry:", 1)[1].strip()

        if "exit:" in low:
            exit_ = line.split("exit:", 1)[1].strip()

    return redundant, entry, exit_


# -----------------------------
# [D] Mermaid ë¼ì¸ ì •ë¦¬
# -----------------------------
def normalize_line(line: str) -> str:
    """
    - ì—£ì§€ ë¼ë²¨ |text| ì œê±° (A -->|label| B)
    - ì•ë’¤ ê³µë°± ì œê±°
    """
    line = re.sub(r"\|[^|]*\|", "", line)  # |...| ì œê±°
    return line.strip()


# -----------------------------
# [E] Mermaid -> (edges, labels) íŒŒì‹±
# -----------------------------
def parse_mermaid_edges_and_labels(mermaid_text: str) -> Tuple[List[Tuple[str, str]], Dict[str, str]]:
    """
    ë°˜í™˜:
    - edges: [(A,B), (B,C), ...]  (ë°©í–¥ ê·¸ë˜í”„)
    - labels: {A:"User", B:"Gateway", ...}
    """
    labels: Dict[str, str] = {}
    edges: List[Tuple[str, str]] = []

    # (1) ë¼ë²¨ ì •ì˜ ìˆ˜ì§‘: A[Gateway] í˜•íƒœ
    for m in LABEL_DEF_RE.finditer(mermaid_text):
        node_id = m.group(1)
        raw_label = m.group(2).strip("[](){}")
        labels[node_id] = raw_label

    # (2) ì—£ì§€ ìˆ˜ì§‘
    for raw in mermaid_text.splitlines():
        if COMMENT_LINE_RE.match(raw):  # %% ì£¼ì„ë¼ì¸ì€ ì—£ì§€ë¡œ ë³´ì§€ ì•ŠìŒ
            continue

        line = raw.strip()
        if not line:
            continue

        low = line.lower()

        # Mermaid ì„ ì–¸/ì„œë¸Œê·¸ë˜í”„ í‚¤ì›Œë“œëŠ” MVPì—ì„œ ë¬´ì‹œ
        if low.startswith("graph ") or low.startswith("flowchart "):
            continue
        if low.startswith("subgraph") or low == "end":
            continue

        line = normalize_line(line)
        if not line:
            continue

        # í•œ ì¤„ì— ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ì—¬ëŸ¬ ë¬¸ì¥ì´ ë¶™ëŠ” ê²½ìš° ë¶„ë¦¬
        parts = [p.strip() for p in line.split(";") if p.strip()]

        for p in parts:
            # (2-a) ê°€ì¥ ê¸°ë³¸ ì¼€ì´ìŠ¤: A --> B
            m = EDGE_LINE_RE.match(p)
            if m:
                a, b = m.group(1), m.group(2)
                edges.append((a, b))
                continue

            # (2-b) ì²´ì¸ ì¼€ì´ìŠ¤: A --> B --> C
            if "-->" in p or "->" in p:
                # í™”ì‚´í‘œ ë³€í˜•ë“¤ì„ í†µì¼í•´ì„œ split í•˜ê¸°
                tmp = re.sub(r"-{1,3}\.?->", " -> ", p)
                tokens = [t.strip() for t in tmp.split("->") if t.strip()]

                chain_nodes: List[str] = []
                for t in tokens:
                    # í† í°ì—ì„œ ì²« ë²ˆì§¸ node idë§Œ ë½‘ìŒ(ë‹¨ìˆœ MVP)
                    found = NODE_ID_RE.findall(t)
                    if found:
                        chain_nodes.append(found[0])

                for i in range(len(chain_nodes) - 1):
                    edges.append((chain_nodes[i], chain_nodes[i + 1]))

    # (3) ì—£ì§€ ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    uniq_edges: List[Tuple[str, str]] = []
    for e in edges:
        if e not in seen:
            uniq_edges.append(e)
            seen.add(e)

    return uniq_edges, labels


# -----------------------------
# [F] entry/exit ì¶”ì •
# -----------------------------
def choose_entry_exit(
    G: nx.DiGraph,
    labels: Dict[str, str],
    entry_hint: Optional[str],
    exit_hint: Optional[str]
) -> Tuple[Optional[str], List[str]]:
    """
    entry/exitê°€ íŒíŠ¸ë¡œ ì£¼ì–´ì§€ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    ì—†ìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹±:
    - entry: in_degree==0 ë…¸ë“œ ì¤‘ (U/User ë¼ë²¨ ìš°ì„ ), ì—†ìœ¼ë©´ ì²« í›„ë³´
    - exit: out_degree==0 ë…¸ë“œë“¤ (ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ)
    """
    entry: Optional[str] = None
    exits: List[str] = []

    # entry ê²°ì •
    if entry_hint and entry_hint in G:
        entry = entry_hint
    else:
        candidates = [n for n in G.nodes if G.in_degree(n) == 0]
        for n in candidates:
            lab = (labels.get(n, "") or "").lower()
            if n.lower() in ("u", "user") or "user" in lab:
                entry = n
                break
        if not entry and candidates:
            entry = candidates[0]

    # exit ê²°ì •
    if exit_hint and exit_hint in G:
        exits = [exit_hint]
    else:
        exits = [n for n in G.nodes if G.out_degree(n) == 0]

        # out_degree==0ê°€ ì—†ìœ¼ë©´ ë¼ë²¨ í‚¤ì›Œë“œë¡œ ì¶”ì •(ìµœí›„ ìˆ˜ë‹¨)
        if not exits:
            key = ["db", "database", "vector", "llm", "model"]
            for n in G.nodes:
                lab = (labels.get(n, "") or "").lower()
                if any(k in lab for k in key):
                    exits.append(n)

    return entry, exits


# -----------------------------
# [G] core ì„œë¸Œê·¸ë˜í”„(í•µì‹¬ ê²½ë¡œ)ë§Œ ì¶”ì¶œ
# -----------------------------
def core_subgraph_nodes(G: nx.DiGraph, entry: Optional[str], exits: List[str]) -> Set[str]:
    """
    entry -> exit ë¡œ ì´ì–´ì§€ëŠ” ê²½ë¡œì— ì°¸ì—¬ ê°€ëŠ¥í•œ ë…¸ë“œë“¤ë§Œ ëª¨ì•„ì„œ coreë¡œ ì‚¼ìŒ.
    ë…¸ì´ì¦ˆ ì œê±°ìš©.
    """
    if not entry or entry not in G or not exits:
        return set(G.nodes)

    reachable_from_entry = nx.descendants(G, entry) | {entry}
    RG = G.reverse(copy=False)

    core: Set[str] = set()
    for ex in exits:
        if ex not in G:
            continue
        can_reach_ex = nx.descendants(RG, ex) | {ex}
        core |= (reachable_from_entry & can_reach_ex)

    return core if core else set(G.nodes)


# -----------------------------
# [H] SPOF ê³„ì‚°(ë‹¨ì ˆì  ê¸°ë°˜)
# -----------------------------
def compute_spof(
    G: nx.DiGraph,
    entry: Optional[str],
    exits: List[str],
    core_nodes: Set[str],
    redundant: Set[str]
) -> List[str]:
    """
    SPOF í›„ë³´:
    - core ì„œë¸Œê·¸ë˜í”„ë¥¼ undirectedë¡œ ë³´ê³  articulation points(ë‹¨ì ˆì ) í›„ë³´ë¥¼ êµ¬í•¨
    - entry/exits/redundant ì œì™¸
    - ì‹¤ì œë¡œ entry->exit ê²½ë¡œë¥¼ ëŠëŠ”ì§€ ê²€ì¦
    """
    if not entry or entry not in G or not exits:
        return []

    H = G.subgraph(core_nodes).copy()
    UG = H.to_undirected()

    candidates = set(nx.articulation_points(UG))  # ê·¸ë˜í”„ ì´ë¡ ì˜ ë‹¨ì ˆì  í›„ë³´
    candidates -= {entry}
    candidates -= set(exits)
    candidates -= set(redundant)

    spofs: List[str] = []
    for node in candidates:
        H2 = H.copy()
        if node not in H2:
            continue
        H2.remove_node(node)

        # exits ì¤‘ í•˜ë‚˜ë¼ë„ ëŠê¸°ë©´ SPOFë¡œ ê°„ì£¼(ë³´ìˆ˜ì ìœ¼ë¡œ)
        cut = False
        for ex in exits:
            if ex not in H2:
                cut = True
                break
            if not nx.has_path(H2, entry, ex):
                cut = True
                break

        if cut:
            spofs.append(node)

    return spofs


# -----------------------------
# [I] ë³‘ëª© í›„ë³´ ê³„ì‚°(ì¤‘ì•™ì„± + fan-in/out)
# -----------------------------
def compute_bottlenecks(
    G: nx.DiGraph,
    core_nodes: Set[str],
    labels: Dict[str, str],
    topk: int = 3
) -> List[Dict]:
    """
    ë³‘ëª© í›„ë³´:
    - betweenness centrality(ì¤‘ì•™ì„±): ê²½ë¡œê°€ ë§ì´ ì§€ë‚˜ê°€ëŠ” ë…¸ë“œì¼ìˆ˜ë¡ í¼
    - fan-in: ë“¤ì–´ì˜¤ëŠ” ì—£ì§€ê°€ ë§ìœ¼ë©´ ìš”ì²­ì´ ëª°ë¦´ ê°€ëŠ¥ì„±â†‘
    - stateful(DB/Queue/Redis ë“±)ì€ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆ ë¶€ì—¬
    """
    H = G.subgraph(core_nodes).copy()
    if H.number_of_nodes() == 0:
        return []

    bc = nx.betweenness_centrality(H.to_undirected(), normalized=True)

    stateful_keys = ["db", "database", "vector", "redis", "queue", "kafka", "mq", "cache"]
    scored = []

    for n in H.nodes:
        lab = (labels.get(n, "") or "").lower()
        fanin = H.in_degree(n)
        fanout = H.out_degree(n)

        bonus = 0.0
        if any(k in lab for k in stateful_keys):
            bonus += 0.20  # ìƒíƒœful ì»´í¬ë„ŒíŠ¸ëŠ” ë³‘ëª©/ë¦¬ìŠ¤í¬ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë‹ˆ ê°€ì¤‘

        # ì ìˆ˜: ì¤‘ì•™ì„± + íŒ¬ì¸/íŒ¬ì•„ì›ƒ + ë³´ë„ˆìŠ¤
        score = bc.get(n, 0.0) + 0.06 * fanin + 0.02 * fanout + bonus
        scored.append((n, score, fanin, fanout, bc.get(n, 0.0)))

    scored.sort(key=lambda x: x[1], reverse=True)

    results: List[Dict] = []
    for n, score, fanin, fanout, bcv in scored[:topk]:
        results.append({
            "node": n,
            "label": labels.get(n),
            "score": round(score, 4),
            "fanin": fanin,
            "fanout": fanout,
            "betweenness": round(bcv, 4),
        })
    return results


# -----------------------------
# [J] ê°ì  ê³„ì‚°
# -----------------------------
def calc_penalties(spofs: List[str], bottlenecks: List[Dict]) -> Dict:
    """
    SPOF/ë³‘ëª© í›„ë³´ ê°œìˆ˜ì— ë”°ë¼ ê°ì  ê³„ì‚°.
    """
    spof_count = len(spofs)
    bottleneck_count = len(bottlenecks)

    spof_penalty = min(SPOF_PENALTY_CAP, spof_count * SPOF_PENALTY_PER)
    bottleneck_penalty = min(BOTTLENECK_PENALTY_CAP, bottleneck_count * BOTTLENECK_PENALTY_PER)

    total_penalty = spof_penalty + bottleneck_penalty

    return {
        "spof_count": spof_count,
        "bottleneck_count": bottleneck_count,
        "spof_penalty": spof_penalty,
        "bottleneck_penalty": bottleneck_penalty,
        "total_penalty": total_penalty,
        "policy": {
            "spof_per": SPOF_PENALTY_PER,
            "spof_cap": SPOF_PENALTY_CAP,
            "bottleneck_per": BOTTLENECK_PENALTY_PER,
            "bottleneck_cap": BOTTLENECK_PENALTY_CAP,
        }
    }


# -----------------------------
# [K] system_results ì—…ë°ì´íŠ¸(ë¶„ì„ ê²°ê³¼ + ê°ì  ë°˜ì˜)
# -----------------------------
def update_system_results(conn, submission_id: int, graph_analysis: dict, penalty_info: dict):
    """
    - ê¸°ì¡´ system_resultsë¥¼ ì½ì–´ì˜¨ ë’¤
    - score_breakdown_jsonì— graph_analysis + penalty metaë¥¼ í•©ì¹˜ê³ 
    - score_totalì„ ê°ì  ë°˜ì˜í•œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    - risk_flags_jsonì— í”Œë˜ê·¸ë¥¼ ì¶”ê°€
    """
    cur = conn.cursor(dictionary=True)

    # (1) ê¸°ì¡´ ê²°ê³¼ row ì¡°íšŒ
    cur.execute(
        "SELECT score_total, score_breakdown_json, risk_flags_json FROM system_results WHERE submission_id=%s",
        (submission_id,)
    )
    row = cur.fetchone()
    if not row:
        raise RuntimeError("system_resultsê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 03_demo_submission_result.sqlì„ ì‹¤í–‰í•´ ê²°ê³¼ rowë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")

    old_score_total = row["score_total"]

    breakdown = row["score_breakdown_json"]
    flags = row["risk_flags_json"]

    # (2) JSON ë¬¸ìì—´ì´ë©´ íŒŒì‹±í•´ì„œ dict/listë¡œ ë°”ê¿ˆ
    if isinstance(breakdown, str):
        breakdown = json.loads(breakdown)
    if isinstance(flags, str):
        flags = json.loads(flags)

    # (3) ê°ì  ì ìš©í•œ ìƒˆ ì ìˆ˜ ê³„ì‚°
    total_penalty = penalty_info["total_penalty"]
    new_score_total = max(0, int(old_score_total) - int(total_penalty))

    # (4) breakdown JSONì— ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ìë¦¬ í™•ë³´
    breakdown.setdefault("items", {})
    breakdown.setdefault("meta", {})

    # (5) graph_analysis ì €ì¥
    breakdown["items"]["graph_analysis"] = graph_analysis

    # (6) ê°ì  ë©”íƒ€ ì €ì¥(ë°œí‘œ/ë””ë²„ê·¸ì— ì•„ì£¼ ìœ ìš©)
    breakdown["meta"]["graph_penalty"] = {
        "old_score_total": old_score_total,
        "new_score_total": new_score_total,
        **penalty_info
    }

    # (7) flags ì—…ë°ì´íŠ¸(ì¤‘ë³µ ì œê±°)
    if not isinstance(flags, list):
        flags = []

    flag_set = set(flags)

    if penalty_info["spof_count"] > 0:
        flag_set.add("SPOF_DETECTED")
        flag_set.add("SCORE_DEDUCTED_FOR_SPOF")

    if penalty_info["bottleneck_count"] > 0:
        flag_set.add("BOTTLENECK_CANDIDATES")
        flag_set.add("SCORE_DEDUCTED_FOR_BOTTLENECKS")

    if total_penalty > 0:
        flag_set.add("GRAPH_PENALTY_APPLIED")

    new_flags = list(flag_set)

    # (8) DB ì—…ë°ì´íŠ¸ ì‹¤í–‰
    cur.execute(
        "UPDATE system_results SET score_total=%s, score_breakdown_json=%s, risk_flags_json=%s WHERE submission_id=%s",
        (
            new_score_total,
            json.dumps(breakdown, ensure_ascii=False),
            json.dumps(new_flags, ensure_ascii=False),
            submission_id
        )
    )
    conn.commit()


# -----------------------------
# [L] main: ìµœê·¼ submission 1ê±´ì„ ë¶„ì„í•´ì„œ ì—…ë°ì´íŠ¸
# -----------------------------
def main():
    # âœ… DB ì ‘ì† ì •ë³´ëŠ” ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ë°”ê¾¸ì„¸ìš”
    conn = mysql.connector.connect(
        host="localhost",
        user="root",
        password="YOUR_PASSWORD",   # <- ì—¬ê¸°ë¥¼ ë³¸ì¸ ë¹„ë°€ë²ˆí˜¸ë¡œ
        database="Engineer_GYM",
    )

    cur = conn.cursor(dictionary=True)

    # (1) ê°€ì¥ ìµœê·¼ ì œì¶œ 1ê±´ ê°€ì ¸ì˜¤ê¸°
    cur.execute("SELECT * FROM system_submissions ORDER BY id DESC LIMIT 1")
    sub = cur.fetchone()
    if not sub:
        print("ë¶„ì„í•  submissionì´ ì—†ìŠµë‹ˆë‹¤. 03ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    submission_id = sub["id"]
    mermaid_text = sub["mermaid_text"]

    # (2) Mermaid ì£¼ì„ íŒíŠ¸ íŒŒì‹±(ì„ íƒ)
    redundant, entry_hint, exit_hint = parse_annotations(mermaid_text)

    # (3) Mermaid -> edges/labels íŒŒì‹±
    edges, labels = parse_mermaid_edges_and_labels(mermaid_text)

    # (4) ë°©í–¥ ê·¸ë˜í”„ ìƒì„±
    G = nx.DiGraph()
    G.add_edges_from(edges)

    # (5) entry/exits ì¶”ì •
    entry, exits = choose_entry_exit(G, labels, entry_hint, exit_hint)

    # (6) í•µì‹¬ ê²½ë¡œ(core)ë§Œ ë½‘ì•„ ë…¸ì´ì¦ˆ ì œê±°
    core = core_subgraph_nodes(G, entry, exits)

    # (7) SPOF í›„ë³´ ê³„ì‚°
    spofs = compute_spof(G, entry, exits, core, redundant)

    # (8) ë³‘ëª© í›„ë³´ ê³„ì‚°(ìƒìœ„ 3ê°œ)
    bottlenecks = compute_bottlenecks(G, core, labels, topk=3)

    # (9) ê°ì  ê³„ì‚°
    penalty_info = calc_penalties(spofs, bottlenecks)

    # (10) graph_analysis JSON ë§Œë“¤ê¸°(ê²°ê³¼ ì €ì¥ìš©)
    graph_analysis = {
        "entry": entry,
        "exits": exits,
        "nodes_cnt": len(G.nodes),
        "edges_cnt": len(G.edges),
        "core_nodes_cnt": len(core),
        "redundant_marked": sorted(list(redundant)),
        "spof_candidates": spofs,
        "bottleneck_candidates": bottlenecks,
        "notes": "MVP: Mermaidë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ì ˆì (SPOF)/ì¤‘ì•™ì„±(ë³‘ëª© í›„ë³´)ì„ ê³„ì‚°í•˜ê³  ì ìˆ˜ ê°ì ì— ë°˜ì˜"
    }

    # (11) system_results ì—…ë°ì´íŠ¸(ë¶„ì„ + ê°ì  ë°˜ì˜)
    update_system_results(conn, submission_id, graph_analysis, penalty_info)

    # (12) ë¡œê·¸ ì¶œë ¥(í™•ì¸ìš©)
    print("[OK] system_results ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    print("submission_id =", submission_id)
    print(json.dumps({"graph_analysis": graph_analysis, "penalty_info": penalty_info}, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
